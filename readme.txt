Github repository: https://github.com/hotshot07/DeeperMind

Overleaf report: https://www.overleaf.com/read/xcgkqqjctrgb

Meeting minutes: https://docs.google.com/document/d/11WZjMbsCun1tGQPkfc7czqA9zyP-bjkqtW2eyNKAf4I/edit?usp=sharing

Video link: https://youtu.be/dJSFqVQXTfY 

Contributions:
Claudia:
- Q-Learning Algo implementation and training
- Generated tables for connect 3 p1 and p2
- Wrote the Minimax part in the report in the algorithms section
- Wrote the lit review part for q-learning
- Wrote script for slide 2
- Optimised Q-table to give the bottom row and middle column state a high q-value to improve the agent's chances of winning

Mark:
- Created/refactored game code logic
- Created arena code
- Created BFS and DFS Implementation
- Created Minimax Implementation
- Exported recorded moves to csv for model training.
- Wrote Abstract, Introduction, Task Environment

Mayank:
- Investigated the different approaches for implementing algorithms for Connect 4
- Help in abstracting code and fixing bugs
- Implementation of Deep Neural Network for DFS/BFS/Minmax and Hybrid Agent
- Plotting graphs from CSVs for game results 
- Wrote Implemetation of BFS/DFS, DNN and helped in evaluating results in report     

Aditya
- Implemented Q-learning algorithm with Claudia & wrote training and arena classes
- Implemented Epsilon decay and reward function to prevent opponent winning
- Trained Q-learning and generated Q-tables for connect 4 player 1 and player 2
- Researched deep Q-learning, tried an implementation on Kaggle
- Wrote Q-learning algorithm description in section 3. Wrote Q-learning multi-objective reward function in related work. Wrote Q-learning results and discssion with Mayank
- Wrote script for slide 3 on evaluation